from kafka import KafkaConsumer
from time import sleep
from json import dumps,loads
from s3fs import S3FileSystem
import boto3
import json
from botocore.exceptions import NoCredentialsError
import toml 
from datetime import datetime


def convert_date_format(payload):
    
    if 'time' in payload:
        timestamp_milliseconds = payload['time'] 
        timestamp_seconds = timestamp_milliseconds / 1000.0
        timestamp_time = datetime.utcfromtimestamp(timestamp_seconds)
        payload['time'] = timestamp_time.strftime('%d/%m/%Y %H:%M:%S')
        
    if 'event_time' in payload:
        timestamp_milliseconds = payload['event_time'] 
        timestamp_seconds = timestamp_milliseconds / 1000.0
        timestamp_time = datetime.utcfromtimestamp(timestamp_seconds)
        payload['event_time'] =timestamp_time.strftime('%d/%m/%Y %H:%M:%S') 

    return payload


# Configure AWS credentials
app_config = toml.load('config.toml') #loading aws configuration files
access_key = app_config['s3']['keyid'] #getting access key id from config.toml file
secret_access_key = app_config['s3']['keysecret'] #getting access key secrets from config.toml file

payload= []
# Configure  region of S3
region = 'us-east-2'

# Create an instance of the S3 client
s3_client = boto3.client('s3',aws_access_key_id= access_key , aws_secret_access_key=secret_access_key ,region_name= region)
s3= S3FileSystem()


# Configurer le consumer de Kafka
consumer = KafkaConsumer(
    'dbserver1.public.ratings',  # 
    bootstrap_servers='localhost:29092',  #  
    group_id='my_group',  # Cambia esto con el ID de tu grupo de consumidores
    auto_offset_reset='earliest',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

# Fields
fields = ['user_id','movieid', 'rating','timestamp','event_time']

# Iterate through the elements in consumer for count, i in enumerate(consumer):
for count,msg in enumerate(consumer):
    # try:

        # Generate file path in S3
        key = "bucket-kafka-stream/rating_kafka_pipeline{}.json".format(count)

        # Convert Python object to JSON format

        payload = msg.value 
        fields = {field: payload['payload']['after'][field] for field in fields if field in payload['payload']['after']}
        fields = convert_date_format(fields)
        json_data =  json.dumps(fields)

        #Upload JSON file to S3
        s3_client.put_object(Bucket="bucket-kafka-pipeline" ,Key = key, Body=json_data)

        print(f"File {key} successfully stored in S3")
"""
    except NoCredentialsError:

        print("No valid credentials will be found to access S3.")
    except Exception as e:
        print(f"Error storing the file {key} in S3: {e}")

    except KeyboardInterrupt:
        pass

    finally:
        # Cierra el consumidor
        consumer.close()
"""

